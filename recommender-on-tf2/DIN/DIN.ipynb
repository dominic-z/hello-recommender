{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_emb.shape (10, 18)\n",
      "user_behaviors_emb.shape (10, 7, 12)\n",
      "ad_emb.shape (10, 12)\n",
      "context_emb.shape (10, 18)\n",
      "used_weights.shape (16, 6)\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,user_feat_num,user_field_num,\n",
    "                 ad_feat_num,ad_field_num,\n",
    "                 context_feat_num,context_field_num,emb_dim,*args,**kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        :param user_feat_num: int 每个用户样本的特征的维度，假如每个样本可以被表征为[0,0,0,1,0,1,2]，那user_feat_num就应该是7\n",
    "        :param user_field_num: int 每个用户样本的特征的field_num，可以理解为有多少种特征，例如一个样本有性别和年龄两类特征(特征向量可能为[0,1,12])，那field_num就是2\n",
    "        :param ad_feat_num: int 每个广告样本的特征的维度，与user_feat_num意义类似\n",
    "        :param ad_field_num: int 每个广告样本的特征的field_num，可以理解为有多少种特征（例如类别、尺寸等等），与user_field_num类似\n",
    "        :param context_feat_num: int 每个上下文样本的特征的维度，与user_feat_num意义类似\n",
    "        :param context_field_num: int 每个上下文样本的特征的field_num，可以理解为有多少种特征（例如时间等等），与user_field_num类似\n",
    "        :param emb_dim: int 嵌入向量维度\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        super(EmbeddingLayer,self).__init__(*args,**kwargs)\n",
    "        self.user_feat_num=user_feat_num\n",
    "        self.user_field_num=user_field_num\n",
    "        self.ad_feat_num=ad_feat_num\n",
    "        self.ad_field_num=ad_field_num\n",
    "        self.context_feat_num=context_feat_num\n",
    "        self.context_field_num=context_field_num\n",
    "        self.emb_dim=emb_dim\n",
    "\n",
    "        self.user_feat_emb_layer=tf.keras.layers.Embedding(input_dim=self.user_feat_num+1,output_dim=self.emb_dim)\n",
    "        self.ad_feat_emb_layer=tf.keras.layers.Embedding(input_dim=self.ad_feat_num+1,output_dim=self.emb_dim)\n",
    "        self.context_feat_emb_layer=tf.keras.layers.Embedding(input_dim=self.context_feat_num+1,output_dim=self.emb_dim)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        :param inputs: a batch of data 由四部分组成  (user_feat_batch,user_behaviors_batch,ad_feat_batch,context_feat_batch)\n",
    "            user_feat_batch: [batch_size, user_field_num] 用户特征batch，[:,i]对应用户的第i个field的特征取值\n",
    "            user_behaviors_batch: [batch_size, seq_len, ad_field_dim] 用户历史行为batch，历史行为指的就是这个用户历史点击过的广告序列，因此最后一个维度是ad_field_dim\n",
    "            ad_feat_batch: [batch_size, ad_field_dim] 广告特征batch\n",
    "            context_feat_batch: [batch_size, context_field_num] 上下文特征batch\n",
    "        :param kwargs:\n",
    "        :return: (user_emb,user_behaviors_emb,ad_emb,context_emb) 返回四份嵌入向量\n",
    "        \"\"\"\n",
    "        # user_feat_batch: [batch_size, user_field_num]\n",
    "        # user_behaviors_batch: [batch_size, seq_len, ad_field_dim]\n",
    "        # ad_feat_batch: [batch_size, ad_field_dim]\n",
    "        # context_feat_batch: [batch_size, context_field_num]\n",
    "        user_feat_batch,user_behaviors_batch,ad_feat_batch,context_feat_batch=inputs\n",
    "\n",
    "        user_emb=self.user_feat_emb_layer(user_feat_batch) # [batch_size, user_field_num, emb_dim]\n",
    "        user_emb=tf.reshape(user_emb,shape=[-1,self.user_field_num*self.emb_dim]) # [batch_size, user_field_num * emb_dim]\n",
    "\n",
    "        user_behaviors_emb=self.ad_feat_emb_layer(user_behaviors_batch) # [batch_size, seq_len, ad_field_dim, emb_dim]\n",
    "        seq_len=user_behaviors_batch.shape[1]\n",
    "        user_behaviors_emb=tf.reshape(user_behaviors_emb,shape=[-1, seq_len,self.ad_field_num*self.emb_dim]) # [batch_size, seq_len, ad_field_dim * emb_dim]\n",
    "\n",
    "        ad_emb=self.ad_feat_emb_layer(ad_feat_batch) # [batch_size, ad_field_num, emb_dim]\n",
    "        ad_emb=tf.reshape(ad_emb,shape=[-1,self.ad_field_num*self.emb_dim]) # [batch_size, ad_field_num * emb_dim]\n",
    "\n",
    "        context_emb=self.context_feat_emb_layer(context_feat_batch) # [batch_size, context_field_num, emb_dim]\n",
    "        context_emb=tf.reshape(context_emb,shape=[-1,self.context_field_num*self.emb_dim]) # [batch_size, context_field_num * emb_dim]\n",
    "\n",
    "        return (user_emb,user_behaviors_emb,ad_emb,context_emb)\n",
    "\n",
    "    def get_used_emb_weights(self,inputs):\n",
    "        \"\"\"\n",
    "        根据每个batch的输入 获取嵌入向量，并且铺平，但与call方法不同的是，如果一个特征出现过一次，他的emb——vec只会计算一次\n",
    "        举例来讲，如果两个用户样本的性别都是男，那么只会计算一个性别男对应的emb_vec，而不是计算两个\n",
    "        这个功能是为了实现paper中提到的部分正则化，即每次训练只用batch之中出现的特征的emb_vec进行正则\n",
    "        :param inputs: a batch of data 由四部分组成  (user_feat_batch,user_behaviors_batch,ad_feat_batch,context_feat_batch)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # user_feat_batch: [batch_size, user_field_num]\n",
    "        # user_behaviors_batch: [batch_size, seq_len, ad_field_dim]\n",
    "        # ad_feat_batch: [batch_size, ad_field_dim]\n",
    "        # context_feat_batch: [batch_size, context_field_num]\n",
    "        user_feat_batch,user_behaviors_batch,ad_feat_batch,context_feat_batch=inputs\n",
    "\n",
    "        unique_user_feat,_=tf.unique(tf.reshape(user_feat_batch, shape=[-1,])) # [unique_num,]\n",
    "        unique_user_emb=self.user_feat_emb_layer(unique_user_feat) # [unique_num, emb_dim]\n",
    "\n",
    "        arr_idx = tf.where(tf.not_equal(user_behaviors_batch, 0))\n",
    "        arr_values=tf.gather_nd(user_behaviors_batch, arr_idx)\n",
    "        unique_user_behaviors,_=tf.unique(arr_values) # [unique_num,]\n",
    "        unique_ad_feat,_=tf.unique(tf.reshape(ad_feat_batch, shape=[-1,])) # [unique_num,]\n",
    "        unique_user_behaviors_ad_emb=self.ad_feat_emb_layer(tf.concat((unique_user_behaviors,unique_ad_feat),axis=0)) # [unique_num, emb_dim]\n",
    "\n",
    "        unique_context_feat,_=tf.unique(tf.reshape(context_feat_batch,shape=[-1,])) # [unique_num,]\n",
    "        unique_context_emb=self.context_feat_emb_layer(unique_context_feat) # [unique_num, emb_dim]\n",
    "        return tf.concat((unique_user_emb,unique_user_behaviors_ad_emb,unique_context_emb),axis=0)\n",
    "\n",
    "\n",
    "user_feat_num=3\n",
    "user_field_num=3\n",
    "ad_feat_num=4\n",
    "ad_field_num=2\n",
    "context_feat_num=5\n",
    "context_field_num=3\n",
    "emb_dim=6\n",
    "batch_size=10\n",
    "seq_len=7\n",
    "\n",
    "user_feat_batch=np.random.randint(1,1+user_feat_num,size=[batch_size,user_field_num]).astype(np.int32)\n",
    "\n",
    "user_behaviors=[]\n",
    "for _ in range(batch_size):\n",
    "    seq=np.random.choice(range(1,1+ad_feat_num),size=[np.random.randint(1,1+seq_len),2]).astype(np.int32).tolist()\n",
    "    user_behaviors.append(seq)\n",
    "user_behaviors=tf.keras.preprocessing.sequence.pad_sequences(user_behaviors,maxlen=seq_len,padding=\"post\")\n",
    "\n",
    "ad_feat_batch=np.random.randint(1,1+ad_feat_num,size=[batch_size,ad_field_num]).astype(np.int32)\n",
    "context_feat_batch=np.random.randint(1,1+context_feat_num,size=[batch_size,context_field_num]).astype(np.int32)\n",
    "\n",
    "emb_layer=EmbeddingLayer(user_feat_num=user_feat_num,\n",
    "user_field_num=user_field_num,\n",
    "ad_feat_num=ad_feat_num,\n",
    "ad_field_num=ad_field_num,\n",
    "context_feat_num=context_feat_num,\n",
    "context_field_num=context_field_num,emb_dim=emb_dim)\n",
    "\n",
    "inputs=(user_feat_batch,user_behaviors,ad_feat_batch,context_feat_batch)\n",
    "user_emb,user_behaviors_emb,ad_emb,context_emb=emb_layer(inputs)\n",
    "used_weights=emb_layer.get_used_emb_weights(inputs)\n",
    "print(\"user_emb.shape\",user_emb.shape)\n",
    "print(\"user_behaviors_emb.shape\",user_behaviors_emb.shape)\n",
    "print(\"ad_emb.shape\",ad_emb.shape)\n",
    "print(\"context_emb.shape\",context_emb.shape)\n",
    "print(\"used_weights.shape\", used_weights.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 12)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "class PRelu:\n",
    "    def __init__(self,alpha=0.2):\n",
    "        self.alpha=alpha\n",
    "\n",
    "    def __call__(self,x, *args, **kwargs):\n",
    "        return tf.nn.leaky_relu(x,alpha=self.alpha)\n",
    "\n",
    "class Dice:\n",
    "    def __init__(self,epsilon=10e-8,alpha=0.2,decay=0.99):\n",
    "        self.epsilon=epsilon\n",
    "        self.alpha=alpha\n",
    "        self.variable_mean=None\n",
    "        self.variable_var=None\n",
    "        self.ema=tf.train.ExponentialMovingAverage(decay=decay)\n",
    "\n",
    "    def __call__(self,x,training=True, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        此处不可以使用tf的batch_norm，\n",
    "        因为tf的bn只能使用于一个轴，例如输入是三个轴的[a,b,c]，使用bn之后相当于对[:,:,0]整体求均值获得一个值，然后用到[:,:,0]的变量上\n",
    "        但是在DIN中，Dice输入的如果是序列，应该是[:,0,0]求均值，然后应用到[:,0,0]上才对，所以应该仅对一个轴分别求均值，所以手动求均值才行\n",
    "\n",
    "        :param x:\n",
    "        :param training:\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # TODO: 但这块的实现还是有问题，padding位会参与均值方差，均值好改，方差不好改啊\n",
    "        mean=tf.reduce_mean(x,axis=0,keepdims=True) # 根据batch计算平均，在在InterestLayer中，结果为[1, seq_len, emb_dim*3]\n",
    "        var=tf.math.reduce_variance(x,axis=0,keepdims=True) # 根据batch计算平均，在InterestLayer中，结果为[1, seq_len, emb_dim*3]\n",
    "        if self.variable_mean is None:\n",
    "            print(\"initialize variable_mean and variable_var\")\n",
    "            self.variable_mean=tf.Variable(tf.zeros_like(mean))\n",
    "            self.variable_var=tf.Variable(tf.zeros_like(var))\n",
    "        if training:\n",
    "            self.variable_mean.assign(mean)\n",
    "            self.variable_var.assign(var)\n",
    "            self.ema.apply([self.variable_mean,self.variable_var])\n",
    "        else:\n",
    "            mean=self.ema.average(self.variable_mean)\n",
    "            var=self.ema.average(self.variable_var)\n",
    "        p=-tf.divide(x-mean,tf.sqrt(var+self.epsilon)) # 在InterestLayer中，结果为[batch_size, seq_len, emb_dim*3]\n",
    "        p=tf.nn.sigmoid(p) # 在InterestLayer中，结果为[batch_size, seq_len, emb_dim*3]\n",
    "        return tf.multiply(p,x)+tf.multiply(tf.multiply(self.alpha,1-p),x)\n",
    "\n",
    "class InterestLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,attention_cells,units_list,*args,**kwargs):\n",
    "        super(InterestLayer,self).__init__(*args,**kwargs)\n",
    "        self.dense_layers=[tf.keras.layers.Dense(units=units,activation=None) for units in units_list]\n",
    "        self.attention_cells=attention_cells\n",
    "        self.scoring_layer=tf.keras.layers.Dense(units=1,activation=None)\n",
    "\n",
    "    def call(self, inputs, training=True, **kwargs):\n",
    "        \"\"\"\n",
    "        兴趣层\n",
    "        :param inputs: a batch of data 由三部分组成 user_behaviors,user_behaviors_emb,ad_emb\n",
    "            user_behaviors就是用户历史行为，用来生成mask用的\n",
    "        :param training:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # user_behaviors: [batch_size, seq_len, ad_field_num]\n",
    "        # user_behaviors_emb: [batch_size, seq_len, emb_dim]\n",
    "        # ad_emb: [batch_size, emb_dim]\n",
    "        user_behaviors,user_behaviors_emb,ad_emb=inputs\n",
    "        # mask\n",
    "        mask=tf.expand_dims(tf.where(tf.not_equal(user_behaviors[:,:,0],0),x=1.,y=0.),axis=-1) # [batch_size, seq_len, 1]\n",
    "\n",
    "        # calc attention scores\n",
    "        # seq_len=user_behaviors.shape[1]\n",
    "        # emb_dim=ad_emb.shape[-1]\n",
    "        # ad_emb=tf.reshape(tf.tile(ad_emb,multiples=[1,seq_len]),shape=[-1,seq_len,emb_dim]) #[batch_size, seq_len, emb_dim]\n",
    "        ad_emb=tf.broadcast_to(tf.expand_dims(ad_emb,axis=1),shape=user_behaviors_emb.shape) #[batch_size, seq_len, emb_dim]\n",
    "        out_product=tf.multiply(user_behaviors_emb,ad_emb) #[batch_size, seq_len, emb_dim]\n",
    "\n",
    "        dense_inputs=tf.concat([user_behaviors_emb,out_product,ad_emb],axis=-1) # [batch_size, seq_len, emb_dim*3]\n",
    "        for dense_layer,attention_cell in zip(self.dense_layers,self.attention_cells):\n",
    "            dense_inputs=dense_layer(dense_inputs)\n",
    "            dense_inputs=attention_cell(dense_inputs,training=training)\n",
    "\n",
    "        # dense_inputs: [batch_size, seq_len, units]\n",
    "        attention_scores=self.scoring_layer(dense_inputs) # [batch_size, seq_len, 1]\n",
    "        interest_emb=tf.multiply(attention_scores,user_behaviors_emb) # [batch_size, seq_len, emb_dim]\n",
    "\n",
    "        interest_emb=tf.multiply(interest_emb,mask)\n",
    "        interest_emb=tf.reduce_sum(interest_emb,axis=1) # [batch_size, emb_dim]\n",
    "        return interest_emb\n",
    "\n",
    "\n",
    "inputs=(user_behaviors,user_behaviors_emb,ad_emb)\n",
    "attention_cells=[PRelu()]\n",
    "units_list=[36]\n",
    "interest_layer=InterestLayer(attention_cells=attention_cells,units_list=units_list)\n",
    "interest_emb = interest_layer(inputs)\n",
    "interest_emb = interest_layer(inputs,training=False)\n",
    "print(interest_emb.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize variable_mean and variable_var\n",
      "initialize variable_mean and variable_var\n",
      "initialize variable_mean and variable_var\n",
      "(10, 2)\n"
     ]
    }
   ],
   "source": [
    "class DIN(tf.keras.Model):\n",
    "    def __init__(self,user_feat_num,user_field_num,\n",
    "                 ad_feat_num,ad_field_num,\n",
    "                 context_feat_num,context_field_num,emb_dim,\n",
    "                 dense_units_list,dense_cells,\n",
    "                 attention_units_list,attention_cells,\n",
    "                 *args,**kwargs):\n",
    "        super(DIN,self).__init__(*args,**kwargs)\n",
    "        self.emb_layer=EmbeddingLayer(user_feat_num=user_feat_num,user_field_num=user_field_num,\n",
    "                                      ad_feat_num=ad_feat_num,ad_field_num=ad_field_num,\n",
    "                                      context_feat_num=context_feat_num,context_field_num=context_field_num,\n",
    "                                      emb_dim=emb_dim)\n",
    "        self.interest_layer=InterestLayer(units_list=attention_units_list,attention_cells=attention_cells)\n",
    "        self.dense_layers=[tf.keras.layers.Dense(units=units,activation=None) for units in dense_units_list]\n",
    "        self.dense_cells=dense_cells\n",
    "        self.scoring_layer=tf.keras.layers.Dense(units=2,activation=None)\n",
    "\n",
    "    def call(self, inputs, training=True, mask=None):\n",
    "        _,user_behaviors,_,_=inputs\n",
    "        user_emb,user_behaviors_emb,ad_emb,context_emb=self.emb_layer(inputs)\n",
    "        interest_emb=self.interest_layer((user_behaviors,user_behaviors_emb,ad_emb),training=training)\n",
    "\n",
    "        dense_inputs=tf.concat([user_emb,interest_emb,context_emb],axis=1)\n",
    "        for dense_layer,dense_cell in zip(self.dense_layers,self.dense_cells):\n",
    "            dense_inputs=dense_layer(dense_inputs)\n",
    "            dense_inputs=dense_cell(dense_inputs)\n",
    "        output=self.scoring_layer(dense_inputs)\n",
    "\n",
    "        return output\n",
    "\n",
    "user_feat_num=3\n",
    "user_field_num=3\n",
    "ad_feat_num=4\n",
    "ad_field_num=2\n",
    "context_feat_num=5\n",
    "context_field_num=3\n",
    "emb_dim=6\n",
    "batch_size=10\n",
    "seq_len=7\n",
    "user_feat_batch=np.random.randint(1,1+user_feat_num,size=[batch_size,user_field_num]).astype(np.int32)\n",
    "\n",
    "user_behaviors=[]\n",
    "for _ in range(batch_size):\n",
    "    seq=np.random.choice(range(1,1+ad_feat_num),size=[np.random.randint(1,1+seq_len),2]).astype(np.int32).tolist()\n",
    "    user_behaviors.append(seq)\n",
    "user_behaviors=tf.keras.preprocessing.sequence.pad_sequences(user_behaviors,maxlen=seq_len,padding=\"post\")\n",
    "\n",
    "ad_feat_batch=np.random.randint(1,1+ad_feat_num,size=[batch_size,ad_field_num]).astype(np.int32)\n",
    "context_feat_batch=np.random.randint(1,1+context_feat_num,size=[batch_size,context_field_num]).astype(np.int32)\n",
    "\n",
    "din=DIN(user_feat_num=user_feat_num,\n",
    "user_field_num=user_field_num,\n",
    "ad_feat_num=ad_feat_num,\n",
    "ad_field_num=ad_field_num,\n",
    "context_feat_num=context_feat_num,\n",
    "context_field_num=context_field_num,emb_dim=emb_dim,\n",
    "        attention_units_list=[36],attention_cells=[Dice()],\n",
    "        dense_units_list=[200,80],dense_cells=[Dice(),Dice()])\n",
    "\n",
    "inputs=(user_feat_batch,user_behaviors,ad_feat_batch,context_feat_batch)\n",
    "output=din(inputs)\n",
    "output=din(inputs,training=False)\n",
    "print(output.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}