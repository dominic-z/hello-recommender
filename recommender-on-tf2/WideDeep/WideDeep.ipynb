{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "class WideAndDeep(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    非常简单的wide and deep模型\n",
    "    \"\"\"\n",
    "    def __init__(self,name,emb_input_output_dims,deep_units_list,first_layer_units):\n",
    "        \"\"\"\n",
    "\n",
    "        :param name: string 模型名称\n",
    "        :param emb_input_output_dims: [(int,int)] 用于构建embedding_layer的参数，每个tuple包含一个input_dim和一个output_dim，基于此构建embedding lyaer\n",
    "        :param deep_units_list: [int] 深度模型各个layer的神经元个数\n",
    "        :param first_layer_units: int 最上层的dense层的神经元个数，即输出的维度\n",
    "        \"\"\"\n",
    "        super(WideAndDeep,self).__init__(name=name)\n",
    "        self.var=tf.Variable([1.1]) # 没啥用就是测试一下\n",
    "        self.emb_layers=list()\n",
    "        for input_dim,output_dim in emb_input_output_dims:\n",
    "            self.emb_layers.append(tf.keras.layers.Embedding(input_dim=input_dim,output_dim=output_dim))\n",
    "\n",
    "        self.deep_layers=list()\n",
    "        for units in deep_units_list:\n",
    "            self.deep_layers.append(\n",
    "                tf.keras.layers.Dense(units=units,\n",
    "                                              kernel_initializer=tf.keras.initializers.TruncatedNormal(),\n",
    "                                              bias_initializer=tf.keras.initializers.Ones())\n",
    "            )\n",
    "        self.first_layer=tf.keras.layers.Dense(units=first_layer_units,\n",
    "                                               activation=None,\n",
    "                                              kernel_initializer=tf.keras.initializers.TruncatedNormal(),\n",
    "                                              bias_initializer=tf.keras.initializers.Ones())\n",
    "\n",
    "    def call(self, wide_input_batch,deep_input_batch,deep_emb_input_batch):\n",
    "        \"\"\"\n",
    "\n",
    "        :param wide_input_batch: tensor or a batch of data 广度模型的输入数据batch 不会经过embedding，shape:[batch_size, wide_input_feature_dim]\n",
    "        :param deep_input_batch: tensor or a batch of data 深度模型的不需要embedding的输入数据batch，shape:[batch_size, deep_input_feature_dim]\n",
    "        :param deep_emb_input_batch: tensor or a batch of data 深度模型的不需要embedding的输入数据batch，shape:[batch_size, deep_emb_input_feature_dim]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        emb_outputs=list()\n",
    "        for i,emb_layer in enumerate(self.emb_layers):\n",
    "            emb_outputs.append(emb_layer(deep_emb_input_batch[:,i]))\n",
    "\n",
    "        concat_emb_output=tf.concat(emb_outputs,axis=1)\n",
    "        deep_input=tf.concat([concat_emb_output,deep_input_batch],axis=1)\n",
    "        for deep_layer in self.deep_layers:\n",
    "            deep_input=deep_layer(deep_input)\n",
    "\n",
    "        first_layer_input=tf.concat([wide_input_batch,deep_input],axis=1)\n",
    "        output=self.first_layer(first_layer_input)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "tf.Tensor(\n",
      "[[0.7183332 1.0713701]\n",
      " [0.7878482 1.0301176]\n",
      " [0.7891807 1.0242541]\n",
      " [0.7828526 1.1120307]\n",
      " [0.7718165 1.0623623]], shape=(5, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "wide_input_ds=tf.data.Dataset.from_tensor_slices(np.random.random(size=(10,10)).astype(np.float32))\n",
    "batched_wide_input_ds=wide_input_ds.batch(5)\n",
    "wide_input_batch=next(iter(batched_wide_input_ds))\n",
    "\n",
    "deep_input_ds=tf.data.Dataset.from_tensor_slices(np.random.random(size=(10,10)).astype(np.float32))\n",
    "batched_deep_input_ds=deep_input_ds.batch(5)\n",
    "deep_input_batch=next(iter(batched_deep_input_ds))\n",
    "\n",
    "deep_emb_input_ds=tf.data.Dataset.from_tensor_slices(np.random.randint(0,3,size=(10,1)))\n",
    "batched_deep_emb_input_ds=deep_emb_input_ds.batch(5)\n",
    "deep_emb_input_batch=next(iter(batched_deep_emb_input_ds))\n",
    "\n",
    "wide_deep=WideAndDeep(\"name\",emb_input_output_dims=[(3,2)],deep_units_list=[3,2],first_layer_units=2)\n",
    "wide_deep.trainable=False\n",
    "output=wide_deep(wide_input_batch,deep_input_batch,deep_emb_input_batch)\n",
    "print(wide_deep.trainable_variables)\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([1.1], dtype=float32)>]"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}