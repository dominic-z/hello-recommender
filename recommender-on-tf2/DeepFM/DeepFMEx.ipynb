{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 在原版paper的deepfm算法做了一点改进，即fm_component和deep component输出的都不只是一个数了，而是一个响亮\n",
    "# 大量参照https://github.com/ChenglongChen/tensorflow-DeepFM\n",
    "# 他对deep fm进行了一点点优化，虽然数据描述不太明白，但是其数据结构设计得确实好，很精简也非常适合这个算法\n",
    "# deep fm只能处理one-hot类型的，无法直接处理multi-hot特征\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_indices_arr\n",
      "[[0. 2. 8.]\n",
      " [8. 2. 9.]\n",
      " [2. 4. 3.]\n",
      " [4. 5. 8.]\n",
      " [7. 6. 2.]\n",
      " [1. 6. 3.]\n",
      " [5. 8. 3.]\n",
      " [7. 0. 3.]\n",
      " [9. 6. 3.]]\n",
      "\n",
      "feat_vals_arr\n",
      "[[1.         1.         0.52765805]\n",
      " [1.         1.         0.5158172 ]\n",
      " [1.         1.         0.9801646 ]\n",
      " [1.         1.         0.38281646]\n",
      " [1.         1.         0.8272224 ]\n",
      " [1.         1.         0.5027076 ]\n",
      " [1.         1.         0.3349735 ]\n",
      " [1.         1.         0.9097907 ]\n",
      " [1.         1.         0.9288939 ]]\n",
      "\n",
      "emb vectors\n",
      "tf.Tensor(\n",
      "[[[-0.00363066 -0.03910058  0.02769772 -0.02341567]\n",
      "  [ 0.02214828 -0.01374368 -0.0273025   0.04496919]\n",
      "  [-0.01529532 -0.02506005 -0.00118024  0.02598126]]\n",
      "\n",
      " [[-0.02898718 -0.04749297 -0.00223675  0.04923883]\n",
      "  [ 0.02214828 -0.01374368 -0.0273025   0.04496919]\n",
      "  [-0.02430143  0.00503824 -0.02555656 -0.01632347]]\n",
      "\n",
      " [[ 0.02214828 -0.01374368 -0.0273025   0.04496919]\n",
      "  [ 0.04276675  0.00664122  0.02008495 -0.0024429 ]\n",
      "  [ 0.01792878 -0.0323379  -0.01729286 -0.00096627]]\n",
      "\n",
      " [[ 0.04276675  0.00664122  0.02008495 -0.0024429 ]\n",
      "  [-0.00775075 -0.00512141  0.04000158 -0.01437348]\n",
      "  [-0.01109677 -0.01818109 -0.00085626  0.01884943]]\n",
      "\n",
      " [[ 0.02500394 -0.00966929  0.01939527  0.03760343]\n",
      "  [ 0.0482601   0.0011686   0.04375246 -0.04904955]\n",
      "  [ 0.01832155 -0.01136908 -0.02258524  0.03719952]]], shape=(5, 3, 4), dtype=float32)\n",
      "[<tf.Variable 'embedding_layer_9/embedding_9/embeddings:0' shape=(10, 4) dtype=float32, numpy=\n",
      "array([[-0.00363066, -0.03910058,  0.02769772, -0.02341567],\n",
      "       [-0.02157223, -0.02844602, -0.00919207,  0.02566603],\n",
      "       [ 0.02214828, -0.01374368, -0.0273025 ,  0.04496919],\n",
      "       [ 0.0182916 , -0.03299232, -0.01764281, -0.00098582],\n",
      "       [ 0.04276675,  0.00664122,  0.02008495, -0.0024429 ],\n",
      "       [-0.00775075, -0.00512141,  0.04000158, -0.01437348],\n",
      "       [ 0.0482601 ,  0.0011686 ,  0.04375246, -0.04904955],\n",
      "       [ 0.02500394, -0.00966929,  0.01939527,  0.03760343],\n",
      "       [-0.02898718, -0.04749297, -0.00223675,  0.04923883],\n",
      "       [-0.04711249,  0.0097675 , -0.04954576, -0.03164585]],\n",
      "      dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,feat_dim,field_num,emb_dim,*args,**kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        :param feat_dim: int 每个样本的特征维度，假如每个样本可以被表征为[0,0,0,1,0,1,2]，那feat_dim就应该是7\n",
    "        :param field_num: int 每个样本的特征的field_num，可以理解为有多少种特征，例如一个样本有性别和年龄两类特征(特征向量可能为[0,1,12])，那field_num就是2\n",
    "        :param emb_dim: int 对于每个field的嵌入向量维度\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        super(EmbeddingLayer,self).__init__(*args,**kwargs)\n",
    "        self.feat_dim=feat_dim\n",
    "        self.field_num=field_num\n",
    "        self.emb_dim=emb_dim\n",
    "        self.emb_layer=tf.keras.layers.Embedding(input_dim=feat_dim,output_dim=emb_dim)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        :param inputs: (feat_indices_batch, feat_value_batch)，分为两部分，你可以把他看做一个手动构造的稀疏矩阵\n",
    "            例如，如果feat_indices_batch的数据为[[1,5,9],[2,7,8]]；feat_value_batch的数据为[[1,1,2.3],[1,1,0.98]]\n",
    "            假设第一个样本的特征向量是x，那么x[1]=1, x[5]=1, x[9]=2.3，其余位置取值均为0。\n",
    "            这样构造是因为每个样本都有field_num个field，每个field的取值只有一种（one-hot或者连续值）\n",
    "            也就是说每个样本都有field_num个不为0的特征维度。而deep fm算法的嵌入方法是对每一个field嵌入，不管是不是连续值都要嵌入，然后再乘以特征取值\n",
    "            例如x[9]=2.3，那就要从emb_table里找到第9个emb_vector，然后乘以2.3\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # feat_indices_batch: [batch_size, field_num]\n",
    "        # feat_value_batch: [batch_size, field_num]\n",
    "        feat_indices_batch,feat_value_batch=inputs\n",
    "        # 两者形状要相同，并且二者的第二个轴取值维度都是field_num个\n",
    "        assert feat_indices_batch.shape==feat_value_batch.shape\n",
    "        assert feat_indices_batch.shape[1:]==[self.field_num]\n",
    "\n",
    "        emb_vectors=self.emb_layer(feat_indices_batch) # [batch_size, field_num, emb_dim]\n",
    "        feat_value_batch = tf.expand_dims(feat_value_batch,axis=-1) # [batch_size, field_num, 1]\n",
    "\n",
    "        # broadcast性质 feat_value_batch会被看做[batch_size, field_num, emb_dim]\n",
    "        emb_vectors = tf.multiply(emb_vectors,feat_value_batch) # [batch_size, field_num, emb_dim]\n",
    "        return emb_vectors\n",
    "\n",
    "\n",
    "feat_indices_arr=[np.random.choice(range(10),size=[1,3],replace=False) for _ in range(9)]\n",
    "feat_indices_arr=np.concatenate(feat_indices_arr,axis=0).astype(np.float32)\n",
    "print(\"feat_indices_arr\")\n",
    "print(feat_indices_arr) #[10,3]\n",
    "\n",
    "feat_vals_arr=np.concatenate((np.ones(shape=[9,2]),\n",
    "                              np.random.random(size=[9,1])),axis=1).astype(np.float32)\n",
    "print(\"\\nfeat_vals_arr\")\n",
    "print(feat_vals_arr) # [10,3]\n",
    "\n",
    "input_ds=tf.data.Dataset.from_tensor_slices((feat_indices_arr,feat_vals_arr))\n",
    "batched_ds=input_ds.batch(5)\n",
    "iterator=iter(batched_ds)\n",
    "input_batch=next(iterator)\n",
    "\n",
    "\n",
    "print(\"\\nemb vectors\")\n",
    "emb_layer=EmbeddingLayer(feat_dim=10,field_num=3,emb_dim=4)\n",
    "emb_vectors=emb_layer(input_batch)\n",
    "print(emb_vectors)\n",
    "print(emb_layer.emb_layer.weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "emb vectors\n",
      "tf.Tensor(\n",
      "[[[ 3.9646816e-02  2.3797657e-02 -3.0525697e-02  2.3773909e-03]\n",
      "  [-2.6029622e-02 -2.9312484e-03  2.2400569e-02 -3.9204825e-02]\n",
      "  [ 3.4188058e-02 -4.8973858e-02 -1.5361510e-02  2.7281526e-02]\n",
      "  ...\n",
      "  [ 1.8965114e-02 -2.6497841e-02  8.8852644e-04 -2.8151739e-02]\n",
      "  [ 4.1447770e-02  4.8467841e-02  3.9479081e-02  4.1204941e-02]\n",
      "  [ 1.6248481e-03  1.1679070e-03 -1.9375883e-03  4.2101249e-04]]\n",
      "\n",
      " [[-1.1873804e-02 -2.6591385e-02  4.4963505e-02 -2.1384502e-02]\n",
      "  [ 3.2922849e-03 -1.5337951e-03 -1.8142235e-02  9.0087056e-03]\n",
      "  [ 3.7219848e-02  2.0950604e-02 -3.7559606e-02 -4.2227637e-02]\n",
      "  ...\n",
      "  [ 1.6509663e-02 -2.5190413e-05  4.0651057e-02 -1.7813552e-02]\n",
      "  [-1.8620729e-02 -4.2375028e-02 -3.2706082e-02  1.8036831e-02]\n",
      "  [-8.0271885e-03  1.8071879e-02  1.4420348e-02 -1.6763832e-02]]\n",
      "\n",
      " [[ 1.3585184e-02  2.6681796e-03 -4.4106841e-02  4.6707753e-02]\n",
      "  [ 2.8806534e-02 -2.7917279e-02 -2.5102843e-02 -2.8342057e-02]\n",
      "  [ 3.3310521e-02 -7.3150396e-03 -2.6158893e-02 -2.8007627e-02]\n",
      "  ...\n",
      "  [ 2.7977113e-02 -1.1215448e-02  9.6736066e-03 -2.5039805e-02]\n",
      "  [-2.6853407e-02 -4.4785108e-02 -2.4719620e-02 -9.8552927e-03]\n",
      "  [ 1.7563278e-02  2.3287648e-02 -7.6758345e-03  1.9280195e-02]]\n",
      "\n",
      " [[-1.8545762e-03  2.2486340e-02  1.6797964e-02 -1.6325809e-02]\n",
      "  [ 3.1571414e-02 -4.4732001e-02 -3.5949659e-02  3.7291896e-02]\n",
      "  [-5.1165745e-04  3.4218207e-03 -6.7196488e-03 -1.6921259e-02]\n",
      "  ...\n",
      "  [-2.9015113e-02  1.5362386e-02 -3.4255937e-02  2.6672076e-02]\n",
      "  [-5.6949034e-03  3.7380043e-02  3.9125655e-02 -2.0437611e-02]\n",
      "  [ 1.2648931e-02 -2.6840778e-02  3.6347609e-02 -1.0790418e-02]]\n",
      "\n",
      " [[-9.0493187e-03  4.3611851e-02 -1.2750935e-02 -4.3067791e-02]\n",
      "  [-3.1240249e-02 -2.8220892e-02  6.9082156e-03 -7.7915303e-03]\n",
      "  [ 7.7646747e-03  1.4990341e-02 -3.1263731e-02 -3.0425608e-02]\n",
      "  ...\n",
      "  [-1.4143743e-02 -3.7336100e-02  1.4781106e-02 -2.4810553e-02]\n",
      "  [-1.4169026e-02 -2.6245331e-02 -4.4302475e-02 -6.0885549e-03]\n",
      "  [ 3.3663396e-02 -3.3075635e-03 -2.2634538e-02  1.3546107e-02]]], shape=(5, 3000, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-65.10472    44.28153   124.32819    31.272242 ]\n",
      " [-41.86141     5.2479334 -38.38745   -38.27343  ]\n",
      " [-31.083231   -2.6064205  48.88235    64.16684  ]\n",
      " [ 29.754536   21.778358   29.28876     5.696401 ]\n",
      " [ 20.328205   -1.992758  -87.084045  -11.775785 ]], shape=(5, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-1.1249189e+00 -6.7239004e-01 -2.4002331e-01  9.4041002e-01]\n",
      " [-1.2179049e+00  8.0037117e-04  3.0821261e+00 -1.5623355e-01]\n",
      " [ 7.8445661e-01 -1.1053209e+00  2.5748119e+00 -8.2977188e-01]\n",
      " [-4.6382469e-01 -3.3672613e-01 -5.7166415e-01 -1.2711748e+00]\n",
      " [-7.8177220e-01  2.0643215e+00  3.8730013e-01 -8.6927426e-01]], shape=(5, 4), dtype=float32)\n",
      "\n",
      "fm_outputs\n",
      "tf.Tensor(\n",
      "[[-6.6229637e+01  4.3609138e+01  1.2408817e+02  3.2212650e+01]\n",
      " [-4.3079311e+01  5.2487335e+00 -3.5305325e+01 -3.8429665e+01]\n",
      " [-3.0298775e+01 -3.7117414e+00  5.1457161e+01  6.3337067e+01]\n",
      " [ 2.9290710e+01  2.1441633e+01  2.8717094e+01  4.4252262e+00]\n",
      " [ 1.9546432e+01  7.1563482e-02 -8.6696747e+01 -1.2645060e+01]], shape=(5, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class FMComponent(tf.keras.layers.Layer):\n",
    "    def __init__(self,feat_dim,field_num,emb_dim,*args,**kwargs):\n",
    "        \"\"\"\n",
    "        同EmbeddingLayer\n",
    "        :param feat_dim:\n",
    "        :param field_num:\n",
    "        :param emb_dim:\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        super(FMComponent,self).__init__(*args,**kwargs)\n",
    "        self.feat_dim=feat_dim\n",
    "        self.field_num=field_num\n",
    "        self.emb_dim=emb_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w=tf.Variable(initial_value=tf.random.truncated_normal(shape=[self.feat_dim, self.emb_dim]))\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        :param inputs: (raw_input_batch,emb_vectors)\n",
    "            其中raw_input_batch是feat_indices_batch,feat_value_batch 其实就是EmbeddingLayer的输入 用于计算一阶term\n",
    "            emb_vectors是后者是emb_layer的输出\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        raw_input_batch,emb_vectors=inputs # emb_vectors: [batch_size, field_num, emb_dim]\n",
    "        # feat_indices_batch: [batch_size, field_num]\n",
    "        # feat_value_batch: [batch_size, field_num]\n",
    "        feat_indices_batch,feat_value_batch=raw_input_batch\n",
    "\n",
    "        # first order term\n",
    "        # 使用feat_indices找到embedding_lookup快速找到field_num个权重然后做相乘\n",
    "        # 例如，如果一个样本x，他在特征维度1、3、5上有取值，那么他的feat_indices=[1,3,5]。那只需要从self.w找到第1、3、5个数就可以了\n",
    "        # 这样的计算方法更加快速\n",
    "        # 改进1 一阶输出也是一个向量而非一个标量，这就要求self.w的shape为[self.feat_dim, self.emb_dim]\n",
    "        # 原本deepFM之中女\n",
    "        # 一阶做的事情实际上就是，假如一个样本有三个field上的取值为[1,1,1.3]，特征id分别是1，3，5，那么一阶结果就是w1*1+w3*1+w5*1.3\n",
    "        # 拓展版本就是将w1换成了一个长度为emb_size的向量\n",
    "        weights=tf.nn.embedding_lookup(params=self.w,ids=tf.cast(feat_indices_batch,tf.int32)) # [batch_size, field_num, self.emb_dim]\n",
    "        # 需要对feat_value_batch扩充一下，不然无法进行broadcast\n",
    "        first_order_term = tf.multiply(tf.expand_dims(feat_value_batch,axis=2),weights) # [batch_size, field_num, emb_dim]\n",
    "        first_order_term = tf.reduce_sum(first_order_term,axis=1) # [batch_size, emb_dim]\n",
    "        print(first_order_term)\n",
    "\n",
    "\n",
    "        # second order term\n",
    "        # 下面这个是fm算法的优化算法 和平方减去平方和\n",
    "        sum_square=tf.square(tf.reduce_sum(emb_vectors,axis=1)) # [batch_size, emb_dim]\n",
    "        square_sum=tf.reduce_sum(tf.square(emb_vectors),axis=1) # [batch_size, emb_dim]\n",
    "\n",
    "        second_order_term=1/2*tf.subtract(sum_square,square_sum) # [batch_size, emb_dim]\n",
    "        print(second_order_term)\n",
    "\n",
    "        y_fm=first_order_term+second_order_term\n",
    "        return y_fm\n",
    "\n",
    "\n",
    "# 突出一个问题，如果num_fields过多，会导致fm_output的数值膨胀\n",
    "num_fields=3000\n",
    "num_features=30000\n",
    "feat_indices_arr=[np.random.choice(range(1,num_features),size=[1,num_fields],replace=False) for _ in range(9)]\n",
    "feat_indices_arr=np.concatenate(feat_indices_arr,axis=0).astype(np.float32)\n",
    "feat_vals_arr=np.concatenate((np.ones(shape=[9,2999]),\n",
    "                              np.random.random(size=[9,1])),axis=1).astype(np.float32)\n",
    "\n",
    "input_ds=tf.data.Dataset.from_tensor_slices((feat_indices_arr,feat_vals_arr))\n",
    "batched_ds=input_ds.batch(5)\n",
    "iterator=iter(batched_ds)\n",
    "input_batch=next(iterator)\n",
    "\n",
    "\n",
    "print(\"\\nemb vectors\")\n",
    "emb_layer=EmbeddingLayer(feat_dim=num_features,field_num=num_fields,emb_dim=4)\n",
    "emb_vectors=emb_layer(input_batch)\n",
    "print(emb_vectors)\n",
    "\n",
    "fm_component=FMComponent(feat_dim=num_features,field_num=num_fields,emb_dim=4)\n",
    "fm_inputs=(input_batch,emb_vectors)\n",
    "fm_outputs=fm_component(fm_inputs)\n",
    "print(\"\\nfm_outputs\")\n",
    "print(fm_outputs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class DeepComponent(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    深层网络，没啥好说的\n",
    "    \"\"\"\n",
    "    def __init__(self,deep_units_list,*args,**kwargs):\n",
    "        super(DeepComponent,self).__init__(*args,**kwargs)\n",
    "        self.deep_layers=list()\n",
    "        for deep_units in deep_units_list:\n",
    "            self.deep_layers.append(tf.keras.layers.Dense(units=deep_units,activation=tf.nn.relu))\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        for deep_layer in self.deep_layers:\n",
    "            inputs=deep_layer(inputs)\n",
    "        return inputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\narray([[-0.26829186, -0.108273  ],\n       [-1.1350449 ,  2.1167572 ],\n       [-1.3979748 ,  1.5567256 ],\n       [-0.7603167 ,  1.079642  ],\n       [-0.16347432, -1.9003195 ]], dtype=float32)>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class DeepFM(tf.keras.Model):\n",
    "    def __init__(self,feat_dim,field_num,emb_dim,deep_units_list,scoring_units=2,*args,**kwargs):\n",
    "        \"\"\"\n",
    "        同EmbeddingLayer描述\n",
    "        :param feat_dim:\n",
    "        :param field_num:\n",
    "        :param emb_dim:\n",
    "        :param deep_units_list:\n",
    "        :param scoring_units: int 输出的类别数目，两类问题就是2，三类问题就是3\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        super(DeepFM,self).__init__(*args,**kwargs)\n",
    "\n",
    "        self.emb_layer=EmbeddingLayer(feat_dim=feat_dim,field_num=field_num,emb_dim=emb_dim)\n",
    "        self.fm_component=FMComponent(feat_dim=feat_dim,field_num=field_num,emb_dim=emb_dim)\n",
    "        self.deep_component=DeepComponent(deep_units_list=deep_units_list)\n",
    "        self.scoring_layer=tf.keras.layers.Dense(units=scoring_units,activation=None)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        emb_vectors=self.emb_layer(inputs)\n",
    "\n",
    "        fm_inputs=(inputs,emb_vectors)\n",
    "        y_fm=self.fm_component(fm_inputs)\n",
    "\n",
    "        deep_inputs=tf.reshape(emb_vectors,shape=[emb_vectors.shape[0],-1])\n",
    "        y_deep=self.deep_component(deep_inputs)\n",
    "        y = self.scoring_layer(tf.concat((y_fm,y_deep),axis=1))\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "feat_indices_arr=[np.random.choice(range(10),size=[1,3],replace=False) for _ in range(9)]\n",
    "feat_indices_arr=np.concatenate(feat_indices_arr,axis=0).astype(np.float32)\n",
    "feat_vals_arr=np.concatenate((np.ones(shape=[9,2]),\n",
    "                              np.random.random(size=[9,1])),axis=1).astype(np.float32)\n",
    "\n",
    "input_ds=tf.data.Dataset.from_tensor_slices((feat_indices_arr,feat_vals_arr))\n",
    "batched_ds=input_ds.batch(5)\n",
    "iterator=iter(batched_ds)\n",
    "input_batch=next(iterator)\n",
    "\n",
    "deep_fm_model=DeepFM(feat_dim=10,field_num=3000,emb_dim=4,deep_units_list=[10,8])\n",
    "deep_fm_model(input_batch)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.67044261 0.75686147]\n",
      " [0.14368512 0.98200156]\n",
      " [0.7414385  0.31037816]]\n",
      "tf.Tensor(\n",
      "[[0.47840872 0.52159128]\n",
      " [0.30188948 0.69811052]\n",
      " [0.60612684 0.39387316]], shape=(3, 2), dtype=float64)\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}